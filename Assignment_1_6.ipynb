{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def init_data_binary(n, m):\n",
    "    '''\n",
    "    Generate random dataset of size n, dimension m, with the binary label \n",
    "    '''\n",
    "    X=np.random.rand(n, m)\n",
    "    y=np.random.rand(n, 1)\n",
    "    for i in range(n):\n",
    "        y[i]=-1 if y[i]<0.5 else 1 # binary conversion\n",
    "    return X,y\n",
    "    \n",
    "def init_data_cont(n, m):\n",
    "    '''\n",
    "    Generate random dataset of size n, dimension m, with continuous label in [0,1]\n",
    "    '''\n",
    "    X=np.random.rand(n, m)\n",
    "    y=np.random.rand(n, 1)\n",
    "    return X,y\n",
    "\n",
    "def init_weights(m):\n",
    "    '''\n",
    "    Generate random weights in [0,1]\n",
    "    '''\n",
    "    return np.random.rand(m, 1)\n",
    "\n",
    "def numerical_grad(func_obj, w, epsilon):\n",
    "    '''\n",
    "    compute the gradient numberically and return\n",
    "    '''\n",
    "    m = w.shape[0] # number of features\n",
    "    grad = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        wp = np.copy(w) # positive direction ?\n",
    "        wn = np.copy(w) # negative direction ?\n",
    "        wp[i] = w[i] + epsilon\n",
    "        wn[i] = w[i] - epsilon\n",
    "        grad[i] = (func_obj(wp) - func_obj(wn)) / (2*epsilon)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_sigmoid(var):\n",
    "    '''\n",
    "    return 1/(1+exp(var))\n",
    "    avoid large overflow\n",
    "    '''\n",
    "    if var>=0:\n",
    "        return np.exp(-var)/(1+np.exp(-var))\n",
    "    else:\n",
    "        return 1/(1+np.exp(var))\n",
    "\n",
    "def logistic_loss(w, X, y, la):\n",
    "    '''\n",
    "    input: w parameter, X variables, y label, la lambda\n",
    "    Assignment 6.1\n",
    "    return f as the loss function value and g as the gradient\n",
    "    '''\n",
    "    n = X.shape[0] # size of dataset\n",
    "    # m = X.shape[1] # number of features\n",
    "    f = 0\n",
    "    g = 0\n",
    "    for i in range(n):\n",
    "        x_cdot_w = np.dot(X[i], w) # w^T x\n",
    "        f = f + np.logaddexp(0, -y[i] * x_cdot_w)\n",
    "        g = g + -y[i] * inverse_sigmoid(y[i]* x_cdot_w) * X[i] # exp(-y_iw^Tx_i)/(1+exp(-y_iw^Tx_i)) * (-y_i x_i)\n",
    "    f = f + la * np.sum(np.multiply(w, w))\n",
    "    g = g + 2 * la * w.reshape(1,-1)\n",
    "    return f,g.reshape(-1, 1)\n",
    "\n",
    "def logistic_loss_vectorized(w, X, y, la):\n",
    "    '''\n",
    "    input: w parameter, X variables, y label, la lambda\n",
    "    Assignment 6.1\n",
    "    return f as the loss function value and g as the gradient\n",
    "    '''\n",
    "    x_mul_w = np.matmul(X, w) # n * 1\n",
    "    y_X_w = np.multiply(y, x_mul_w) # n * 1\n",
    "    f = np.sum(np.logaddexp(0, -y_X_w)) + la * np.sum(np.multiply(w, w))\n",
    "    inv_sigmoid_vector = np.exp(np.minimum(0, -y_X_w))/(1 + np.exp(-y_X_w*np.sign(y_X_w))) # n * 1\n",
    "    y_inv_sigmoid = -1 * np.multiply(y, inv_sigmoid_vector) # n * 1\n",
    "    g =  np.matmul(y_inv_sigmoid.T, X) + 2 * la * w.T\n",
    "    return f,g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Block for Question 6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### logistic_loss simple loop ###\n",
      "# number of cases:  100\n",
      "# number of features:  10\n",
      "# lambda:  1.0\n",
      "# time elapsed:  0.00320196151733\n",
      "# loss value:  [97.51352942]\n",
      "# gradient: \n",
      "[[17.43959978 16.68665159 18.27405113 13.6839395  14.97967404 15.24908041\n",
      "  16.23486807 16.60280774 14.06388353 18.66726558]]\n",
      "### logistic_loss simple loop ###\n",
      "# number of cases:  100\n",
      "# number of features:  100000\n",
      "# lambda:  1.0\n",
      "# time elapsed:  0.300104141235\n",
      "# loss value:  [1359088.89206763]\n",
      "# gradient: \n",
      "[[25.03903844 27.41735439 25.96539733 ... 28.40512077 27.65472097\n",
      "  27.0025078 ]]\n",
      "### logistic_loss simple loop ###\n",
      "# number of cases:  100000\n",
      "# number of features:  10\n",
      "# lambda:  1.0\n",
      "# time elapsed:  2.14091801643\n",
      "# loss value:  [114122.29526568]\n",
      "# gradient: \n",
      "[[18927.09114158 19047.93958542 19371.1410727  19399.91818725\n",
      "  19299.85842125 18780.58301381 18971.16171261 19105.82265923\n",
      "  19320.21006099 19005.64364169]]\n",
      "### logistic_loss compare simple loop version and vectorized version ###\n",
      "# number of cases:  100\n",
      "# number of features:  10\n",
      "# lambda:  1.0\n",
      "## Result of simple loop version ##\n",
      "# time elapsed:  0.0021870136261\n",
      "# loss value:  [123.15964319]\n",
      "# gradient: \n",
      "[[21.24090486 21.94141846 22.57147781 22.51137007 19.64828028 19.46538489\n",
      "  18.51615576 18.04067754 18.59605174 19.0255574 ]]\n",
      "## Result of vectorized version ##\n",
      "# time elapsed:  0.000163078308105\n",
      "# loss value:  123.15964319204097\n",
      "# gradient: \n",
      "[[21.24090486 21.94141846 22.57147781 22.51137007 19.64828028 19.46538489\n",
      "  18.51615576 18.04067754 18.59605174 19.0255574 ]]\n",
      "### logistic_loss compare simple loop version and vectorized version ###\n",
      "# number of cases:  100\n",
      "# number of features:  100000\n",
      "# lambda:  1.0\n",
      "## Result of simple loop version ##\n",
      "# time elapsed:  0.0773501396179\n",
      "# loss value:  [1408554.26019422]\n",
      "# gradient: \n",
      "[[31.95291394 32.35959732 30.92873135 ... 30.49135238 23.15613929\n",
      "  26.06979207]]\n",
      "## Result of vectorized version ##\n",
      "# time elapsed:  0.0108230113983\n",
      "# loss value:  1408554.2601942194\n",
      "# gradient: \n",
      "[[31.95291394 32.35959732 30.92873135 ... 30.49135238 23.15613929\n",
      "  26.06979207]]\n",
      "### logistic_loss compare simple loop version and vectorized version ###\n",
      "# number of cases:  100000\n",
      "# number of features:  10\n",
      "# lambda:  1.0\n",
      "## Result of simple loop version ##\n",
      "# time elapsed:  2.1893851757\n",
      "# loss value:  [153736.91857655]\n",
      "# gradient: \n",
      "[[22485.47991221 22576.14756709 22286.82174181 22529.6253346\n",
      "  22605.0743917  22616.574424   22306.72692962 22557.08324842\n",
      "  22473.51917889 22263.75276726]]\n",
      "## Result of vectorized version ##\n",
      "# time elapsed:  0.0131258964539\n",
      "# loss value:  153736.91857654852\n",
      "# gradient: \n",
      "[[22485.47991221 22576.14756709 22286.82174181 22529.6253346\n",
      "  22605.0743917  22616.574424   22306.72692962 22557.08324842\n",
      "  22473.51917889 22263.75276726]]\n",
      "### logistic_loss compare gradient, derivative version and numerical version ###\n",
      "# number of cases:  100\n",
      "# number of features:  10\n",
      "# lambda:  1.0\n",
      "# epsilon:  0.001\n",
      "## Result of derivative version ##\n",
      "# gradient: \n",
      "[[15.67675556 18.98136617 15.89741988 18.44134192 19.55366298 19.48395109\n",
      "  16.52249829 17.74423066 15.48632422 20.55752747]]\n",
      "## Result of numerical version ##\n",
      "# gradient: \n",
      "[15.67675534 18.98136593 15.89741965 18.4413416  19.55366267 19.48395083\n",
      " 16.52249808 17.7442304  15.48632399 20.55752718]\n"
     ]
    }
   ],
   "source": [
    "# Test Assignment 6.1\n",
    "def test_assignment_6_1(n, m, la):\n",
    "    '''\n",
    "    test the function logistic_loss\n",
    "    n: size of dataset\n",
    "    m: number of features\n",
    "    la: lambda\n",
    "    '''\n",
    "    print('''### logistic_loss simple loop ###''')\n",
    "    print('''# number of cases: '''),\n",
    "    print(n)\n",
    "    print('''# number of features: '''),\n",
    "    print(m)\n",
    "    print('''# lambda: '''),\n",
    "    print(la)\n",
    "    X,y = init_data_binary(n, m)\n",
    "    w = init_weights(m)\n",
    "    start=time.time()\n",
    "    f,g=logistic_loss(w, X, y, la)\n",
    "    end=time.time()\n",
    "    print('''# time elapsed: '''),\n",
    "    print(end-start)\n",
    "    print('''# loss value: '''),\n",
    "    print(f)\n",
    "    print('''# gradient: ''')\n",
    "    print(g.T)\n",
    "\n",
    "def test_assignment_6_1_step7_compare(n, m, la):\n",
    "    print('''### logistic_loss compare simple loop version and vectorized version ###''')\n",
    "    print('''# number of cases: '''),\n",
    "    print(n)\n",
    "    print('''# number of features: '''),\n",
    "    print(m)\n",
    "    print('''# lambda: '''),\n",
    "    print(la)\n",
    "    X,y = init_data_binary(n, m)\n",
    "    w = init_weights(m)\n",
    "    start=time.time()\n",
    "    f1, g1 = logistic_loss(w, X, y, la)\n",
    "    end=time.time()\n",
    "    print('''## Result of simple loop version ##''')\n",
    "    print('''# time elapsed: '''),\n",
    "    print(end-start)\n",
    "    print('''# loss value: '''),\n",
    "    print(f1)\n",
    "    print('''# gradient: ''')\n",
    "    print(g1.T)\n",
    "    start=time.time()\n",
    "    f2, g2 = logistic_loss_vectorized(w, X, y, la)\n",
    "    end=time.time()\n",
    "    print('''## Result of vectorized version ##''')\n",
    "    print('''# time elapsed: '''),\n",
    "    print(end-start)\n",
    "    print('''# loss value: '''),\n",
    "    print(f2)\n",
    "    print('''# gradient: ''')\n",
    "    print(g2)\n",
    "    \n",
    "def test_assignment_6_1_step8_compare(n, m, la, epsilon):\n",
    "    print('''### logistic_loss compare gradient, derivative version and numerical version ###''')\n",
    "    print('''# number of cases: '''),\n",
    "    print(n)\n",
    "    print('''# number of features: '''),\n",
    "    print(m)\n",
    "    print('''# lambda: '''),\n",
    "    print(la)\n",
    "    print('''# epsilon: '''),\n",
    "    print(epsilon)\n",
    "    X,y = init_data_binary(n, m)\n",
    "    w = init_weights(m)\n",
    "    _, g1 = logistic_loss(w, X, y, la)\n",
    "    func_obj = lambda w : logistic_loss_vectorized(w, X, y, la)[0]\n",
    "    g2 = numerical_grad(func_obj, w, epsilon)\n",
    "    print('''## Result of derivative version ##''')\n",
    "    print('''# gradient: ''')\n",
    "    print(g1.T)\n",
    "    print('''## Result of numerical version ##''')\n",
    "    print('''# gradient: ''')\n",
    "    print(g2.T)\n",
    "    \n",
    "n,m,la=100,10,1.0\n",
    "test_assignment_6_1(n,m,la)\n",
    "n,m,la=100,100000,1.0\n",
    "test_assignment_6_1(n,m,la)\n",
    "n,m,la=100000,10,1.0\n",
    "test_assignment_6_1(n,m,la)\n",
    "n,m,la=100,10,1.0\n",
    "test_assignment_6_1_step7_compare(n, m, la)\n",
    "n,m,la=100,100000,1.0\n",
    "test_assignment_6_1_step7_compare(n, m, la)\n",
    "n,m,la=100000,10,1.0\n",
    "test_assignment_6_1_step7_compare(n, m, la)\n",
    "n,m,la,epsilon=100,10,1.0,0.001\n",
    "test_assignment_6_1_step8_compare(n, m, la, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6.2 Hinge Loss/SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(w, X, y, la):\n",
    "    '''\n",
    "    input: w parameter, X variables, y label, la lambda\n",
    "    Assignment 6.2\n",
    "    return f as the loss function value and g as the gradient\n",
    "    '''\n",
    "    n = X.shape[0] # size of dataset\n",
    "    # m = X.shape[1] # number of features\n",
    "    f = 0\n",
    "    g = 0\n",
    "    for i in range(n):\n",
    "        one_minus_y_w_x = 1.0 - y[i]* (np.dot(X[i], w))\n",
    "        if one_minus_y_w_x >=0.0:\n",
    "            f = f + one_minus_y_w_x\n",
    "            g = g + - y[i]*X[i]\n",
    "    f = f + la * np.sum(w*w)\n",
    "    g = g + 2 * la * w.reshape(1,-1)\n",
    "    return f,g.reshape(-1)\n",
    "\n",
    "def hinge_loss_vectorized(w, X, y, la):\n",
    "    '''\n",
    "    input: w parameter, X variables, y label, la lambda\n",
    "    Assignment 6.2\n",
    "    return f as the loss function value and g as the gradient\n",
    "    '''\n",
    "    n = X.shape[0]\n",
    "    X_cdot_w = np.matmul(X, w)\n",
    "    one_minus = 1 - np.multiply(X_cdot_w, y)\n",
    "    f = np.sum(np.maximum(0, one_minus)) + la * np.sum(w*w)\n",
    "    minus_y = -1 * np.multiply(y, np.double(one_minus>0))\n",
    "    g = np.matmul(minus_y.reshape(1,-1),X).reshape(-1,1)  + 2 * la *w.reshape(-1,1)\n",
    "    g = g.reshape(-1)\n",
    "    return f, g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Block for Question 6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### hinge_loss simple loop ###\n",
      "# number of cases:  100\n",
      "# number of features:  10\n",
      "# lambda:  1.0\n",
      "# time elapsed:  0.0014340877533\n",
      "# loss value:  [207.05022177]\n",
      "# gradient: \n",
      "[26.61302737 23.65553052 28.27582656 26.89065468 24.72331345 28.44794822\n",
      " 28.0209767  28.04013649 27.884543   27.84634945]\n",
      "### hinge_loss simple loop ###\n",
      "# number of cases:  100\n",
      "# number of features:  100000\n",
      "# lambda:  1.0\n",
      "# time elapsed:  0.0392060279846\n",
      "# loss value:  [1288272.94689123]\n",
      "# gradient: \n",
      "[26.59113676 23.43390758 26.09910254 ... 24.54239136 26.58656263\n",
      " 25.13508917]\n",
      "### hinge_loss simple loop ###\n",
      "# number of cases:  100000\n",
      "# number of features:  10\n",
      "# lambda:  1.0\n",
      "# time elapsed:  0.805207967758\n",
      "# loss value:  [220874.88007107]\n",
      "# gradient: \n",
      "[25080.48788615 25022.4558949  25166.71296261 24985.14973774\n",
      " 25030.54142605 25162.7541822  25176.5215486  25111.73891757\n",
      " 25135.1852438  25185.4551702 ]\n",
      "### hinge_loss compare simple loop version and vectorized version ###\n",
      "# number of cases:  100\n",
      "# number of features:  10\n",
      "# lambda:  1.0\n",
      "## Result of simple loop version ##\n",
      "# time elapsed:  0.000704050064087\n",
      "# loss value:  [171.82524848]\n",
      "# gradient: \n",
      "[26.42442606 26.28121976 24.11295694 24.94818746 24.45584535 21.96759362\n",
      " 24.31036556 28.65618354 22.68957714 22.06030776]\n",
      "## Result of vectorized version ##\n",
      "# time elapsed:  0.00145220756531\n",
      "# loss value:  171.82524847548285\n",
      "# gradient: \n",
      "[26.42442606 26.28121976 24.11295694 24.94818746 24.45584535 21.96759362\n",
      " 24.31036556 28.65618354 22.68957714 22.06030776]\n",
      "### hinge_loss compare simple loop version and vectorized version ###\n",
      "# number of cases:  100\n",
      "# number of features:  100000\n",
      "# lambda:  1.0\n",
      "## Result of simple loop version ##\n",
      "# time elapsed:  0.0379519462585\n",
      "# loss value:  [1405179.94137911]\n",
      "# gradient: \n",
      "[29.45501037 32.03575523 27.52490586 ... 27.89825345 28.01818424\n",
      " 25.22514993]\n",
      "## Result of vectorized version ##\n",
      "# time elapsed:  0.00891590118408\n",
      "# loss value:  1405179.9413791057\n",
      "# gradient: \n",
      "[29.45501037 32.03575523 27.52490586 ... 27.89825345 28.01818424\n",
      " 25.22514993]\n",
      "### hinge_loss compare simple loop version and vectorized version ###\n",
      "# number of cases:  100000\n",
      "# number of features:  10\n",
      "# lambda:  1.0\n",
      "## Result of simple loop version ##\n",
      "# time elapsed:  0.833822011948\n",
      "# loss value:  [167996.42089316]\n",
      "# gradient: \n",
      "[24861.19520605 24882.93922954 24939.77647716 24854.32370181\n",
      " 24871.37919898 24813.09525779 24874.02960624 24833.0743025\n",
      " 24818.39913418 24982.64236396]\n",
      "## Result of vectorized version ##\n",
      "# time elapsed:  0.0050528049469\n",
      "# loss value:  167996.42089315635\n",
      "# gradient: \n",
      "[24861.19520605 24882.93922954 24939.77647716 24854.32370181\n",
      " 24871.37919898 24813.09525779 24874.02960624 24833.0743025\n",
      " 24818.39913418 24982.64236396]\n",
      "### hinge_loss compare gradient, derivative version and numerical version ###\n",
      "# number of cases:  100\n",
      "# number of features:  10\n",
      "# lambda:  1.0\n",
      "# epsilon:  1e-06\n",
      "## Result of derivative version ##\n",
      "# gradient: \n",
      "[16.41544149 18.8349558  19.27981996 16.16740523 19.19837097 18.51302876\n",
      " 17.96727552 20.58366662 16.9504178  21.51291855]\n",
      "## Result of numerical version ##\n",
      "# gradient: \n",
      "[16.41544149 18.8349558  19.27981997 16.16740522 19.19837097 18.51302876\n",
      " 17.96727552 20.58366661 16.95041779 21.51291854]\n"
     ]
    }
   ],
   "source": [
    "# Test Assignment 6.2\n",
    "def test_assignment_6_2(n, m, la):\n",
    "    '''\n",
    "    test the function hinge_loss\n",
    "    n: size of dataset\n",
    "    m: number of features\n",
    "    la: lambda\n",
    "    '''\n",
    "    print('''### hinge_loss simple loop ###''')\n",
    "    print('''# number of cases: '''),\n",
    "    print(n)\n",
    "    print('''# number of features: '''),\n",
    "    print(m)\n",
    "    print('''# lambda: '''),\n",
    "    print(la)\n",
    "    X,y = init_data_binary(n, m)\n",
    "    w = init_weights(m)\n",
    "    start=time.time()\n",
    "    f,g= hinge_loss(w, X, y, la)\n",
    "    end=time.time()\n",
    "    print('''# time elapsed: '''),\n",
    "    print(end-start)\n",
    "    print('''# loss value: '''),\n",
    "    print(f)\n",
    "    print('''# gradient: ''')\n",
    "    print(g)\n",
    "\n",
    "def test_assignment_6_2_step7_compare(n, m, la):\n",
    "    print('''### hinge_loss compare simple loop version and vectorized version ###''')\n",
    "    print('''# number of cases: '''),\n",
    "    print(n)\n",
    "    print('''# number of features: '''),\n",
    "    print(m)\n",
    "    print('''# lambda: '''),\n",
    "    print(la)\n",
    "    X,y = init_data_binary(n, m)\n",
    "    w = init_weights(m)\n",
    "    start=time.time()\n",
    "    f1, g1 = hinge_loss(w, X, y, la)\n",
    "    end=time.time()\n",
    "    print('''## Result of simple loop version ##''')\n",
    "    print('''# time elapsed: '''),\n",
    "    print(end-start)\n",
    "    print('''# loss value: '''),\n",
    "    print(f1)\n",
    "    print('''# gradient: ''')\n",
    "    print(g1)\n",
    "    start=time.time()\n",
    "    f2, g2 = hinge_loss_vectorized(w, X, y, la)\n",
    "    end=time.time()\n",
    "    print('''## Result of vectorized version ##''')\n",
    "    print('''# time elapsed: '''),\n",
    "    print(end-start)\n",
    "    print('''# loss value: '''),\n",
    "    print(f2)\n",
    "    print('''# gradient: ''')\n",
    "    print(g2)\n",
    "\n",
    "def test_assignment_6_2_step8_compare(n, m, la, epsilon):\n",
    "    print('''### hinge_loss compare gradient, derivative version and numerical version ###''')\n",
    "    print('''# number of cases: '''),\n",
    "    print(n)\n",
    "    print('''# number of features: '''),\n",
    "    print(m)\n",
    "    print('''# lambda: '''),\n",
    "    print(la)\n",
    "    print('''# epsilon: '''),\n",
    "    print(epsilon)\n",
    "    X,y = init_data_binary(n, m)\n",
    "    w = init_weights(m)\n",
    "    _, g1 = hinge_loss_vectorized(w, X, y, la)\n",
    "    func_obj = lambda w : hinge_loss_vectorized(w, X, y, la)[0]\n",
    "    g2 = numerical_grad(func_obj, w, epsilon)\n",
    "    print('''## Result of derivative version ##''')\n",
    "    print('''# gradient: ''')\n",
    "    print(g1)\n",
    "    print('''## Result of numerical version ##''')\n",
    "    print('''# gradient: ''')\n",
    "    print(g2)\n",
    "\n",
    "n,m,la=100,10,1.0\n",
    "test_assignment_6_2(n,m,la)\n",
    "n,m,la=100,100000,1.0\n",
    "test_assignment_6_2(n,m,la)\n",
    "n,m,la=100000,10,1.0\n",
    "test_assignment_6_2(n,m,la)\n",
    "n,m,la=100,10,1.0\n",
    "test_assignment_6_2_step7_compare(n, m, la)\n",
    "n,m,la=100,100000,1.0\n",
    "test_assignment_6_2_step7_compare(n, m, la)\n",
    "n,m,la=100000,10,1.0\n",
    "test_assignment_6_2_step7_compare(n, m, la)\n",
    "n,m,la,epsilon=100,10,1.0,0.000001\n",
    "test_assignment_6_2_step8_compare(n, m, la, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6.3 Simple Two Layers Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_two_layers_loss(w, X, y, la):\n",
    "    '''\n",
    "    input: w parameter, X variables, y label, la lambda\n",
    "    Assignment 6.3\n",
    "    return f as the loss function value and g as the gradient\n",
    "    '''\n",
    "    n = X.shape[0] # size of dataset\n",
    "    # m = X.shape[1] # number of features\n",
    "    f = 0\n",
    "    g = 0\n",
    "    for i in range(n):\n",
    "        x_cdot_w = np.dot(X[i], w)\n",
    "        if x_cdot_w >0.0:\n",
    "            f = f + np.square(y[i]-x_cdot_w)\n",
    "            g = g + 2 * (y[i] - x_cdot_w) * -X[i]\n",
    "        else:\n",
    "            f = f + np.square(y[i])\n",
    "    f = f + la * np.sum(np.multiply(w, w))\n",
    "    g = g + 2 * la * w.reshape(1,-1)\n",
    "    return f,g\n",
    "\n",
    "def simple_two_layers_loss_vectorized(w, X, y, la):\n",
    "    '''\n",
    "    input: w parameter, X variables, y label, la lambda\n",
    "    Assignment 6.3\n",
    "    return f as the loss function value and g as the gradient\n",
    "    '''\n",
    "    n = X.shape[0]\n",
    "    m = w.shape[0]\n",
    "    X_cdot_w = np.matmul(X, w) # n * 1\n",
    "    X_cdot_w = X_cdot_w.reshape(-1 ,1)\n",
    "    zeros = np.zeros(n) \n",
    "    zeros = zeros.reshape(-1,1) # n * 1\n",
    "    X_cdot_w = np.append(X_cdot_w, zeros, axis=1)\n",
    "    X_cdot_w = np.max(X_cdot_w, 1).reshape(-1, 1)\n",
    "    f = np.sum(np.square( y - X_cdot_w)) + la * np.sum(np.multiply(w, w))\n",
    "    aux  = np.double(X_cdot_w>0)\n",
    "    minus_X = -2 * X * np.repeat(aux.reshape(-1, 1), repeats=m, axis=1)\n",
    "    g = np.matmul(minus_X.transpose(), (y - X_cdot_w).reshape(-1, 1)) + 2 * la * w.reshape(-1, 1)\n",
    "    g = g.reshape(1, -1)\n",
    "    return f, g\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Block for Question 6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### simple_two_layers_loss simple loop ###\n",
      "# number of cases:  100\n",
      "# number of features:  10\n",
      "# lambda:  1.0\n",
      "# time elapsed:  0.0022120475769\n",
      "# loss value:  [231.18773541]\n",
      "# gradient: \n",
      "[[151.70479005 129.37382117 125.30059159 155.80508924 145.02955707\n",
      "  140.34168058 143.09365479 159.14077117 135.72555122 142.10143867]]\n",
      "### simple_two_layers_loss simple loop ###\n",
      "# number of cases:  100\n",
      "# number of features:  100000\n",
      "# lambda:  1.0\n",
      "# time elapsed:  0.125385046005\n",
      "# loss value:  [6.24354448e+10]\n",
      "# gradient: \n",
      "[[2461679.10200254 2524508.02920337 2657493.45487811 ... 2620020.86495628\n",
      "  2474358.23012109 2586716.95592618]]\n",
      "### simple_two_layers_loss simple loop ###\n",
      "# number of cases:  100000\n",
      "# number of features:  10\n",
      "# lambda:  1.0\n",
      "# time elapsed:  1.33608102798\n",
      "# loss value:  [447172.81115658]\n",
      "# gradient: \n",
      "[[203214.3677658  214580.33629863 215874.04893315 206489.74855917\n",
      "  205566.17282891 218493.60304622 207852.30486409 206167.13290833\n",
      "  212487.02281908 215937.28636247]]\n",
      "### simple_two_layers_loss compare simple loop version and vectorized version ###\n",
      "# number of cases:  100\n",
      "# number of features:  10\n",
      "# lambda:  1.0\n",
      "## Result of simple loop version ##\n",
      "# time elapsed:  0.00172019004822\n",
      "# loss value:  [541.61194904]\n",
      "# gradient: \n",
      "[[237.73639736 239.33325626 235.26798154 221.59065801 253.57939456\n",
      "  231.82272451 253.97696985 225.26270685 208.27833553 260.18636715]]\n",
      "## Result of vectorized version ##\n",
      "# time elapsed:  0.00011682510376\n",
      "# loss value:  541.6119490395729\n",
      "# gradient: \n",
      "[[237.73639736 239.33325626 235.26798154 221.59065801 253.57939456\n",
      "  231.82272451 253.97696985 225.26270685 208.27833553 260.18636715]]\n",
      "### simple_two_layers_loss compare simple loop version and vectorized version ###\n",
      "# number of cases:  100\n",
      "# number of features:  100000\n",
      "# lambda:  1.0\n",
      "## Result of simple loop version ##\n",
      "# time elapsed:  0.0888640880585\n",
      "# loss value:  [6.23900465e+10]\n",
      "# gradient: \n",
      "[[2619910.2902432  2624465.80303126 2408395.21037899 ... 2547766.05524911\n",
      "  2576277.48068669 2234049.48746018]]\n",
      "## Result of vectorized version ##\n",
      "# time elapsed:  0.182699918747\n",
      "# loss value:  62390046517.21641\n",
      "# gradient: \n",
      "[[2619910.2902432  2624465.80303126 2408395.21037899 ... 2547766.05524911\n",
      "  2576277.48068669 2234049.48746018]]\n",
      "### simple_two_layers_loss compare simple loop version and vectorized version ###\n",
      "# number of cases:  100000\n",
      "# number of features:  10\n",
      "# lambda:  1.0\n",
      "## Result of simple loop version ##\n",
      "# time elapsed:  1.58117198944\n",
      "# loss value:  [480131.94039721]\n",
      "# gradient: \n",
      "[[223301.40161139 210322.90525422 209066.49723957 224246.0920638\n",
      "  216389.78442487 210635.42233871 224636.72828134 223588.40042733\n",
      "  222412.26266129 213203.22847681]]\n",
      "## Result of vectorized version ##\n",
      "# time elapsed:  0.0190320014954\n",
      "# loss value:  480131.9403972098\n",
      "# gradient: \n",
      "[[223301.4016114  210322.90525422 209066.49723958 224246.09206381\n",
      "  216389.78442487 210635.42233871 224636.72828134 223588.40042733\n",
      "  222412.26266128 213203.22847681]]\n",
      "### simple_two_layers_loss compare gradient, derivative version and numerical version ###\n",
      "# number of cases:  100\n",
      "# number of features:  10\n",
      "# lambda:  1.0\n",
      "# epsilon:  1e-06\n",
      "## Result of derivative version ##\n",
      "# gradient: \n",
      "[[239.49519162 240.5567621  276.95471208 235.71684479 262.65390311\n",
      "  237.23009601 268.38296823 239.16137739 253.25895034 234.00618337]]\n",
      "## Result of numerical version ##\n",
      "# gradient: \n",
      "[239.49519158 240.55676204 276.95471209 235.71684477 262.65390306\n",
      " 237.23009593 268.38296804 239.16137752 253.25895024 234.00618346]\n"
     ]
    }
   ],
   "source": [
    "# Test Assignment 6.3\n",
    "def test_assignment_6_3(n, m, la):\n",
    "    '''\n",
    "    test the function simple_two_layers_loss\n",
    "    n: size of dataset\n",
    "    m: number of features\n",
    "    la: lambda\n",
    "    '''\n",
    "    print('''### simple_two_layers_loss simple loop ###''')\n",
    "    print('''# number of cases: '''),\n",
    "    print(n)\n",
    "    print('''# number of features: '''),\n",
    "    print(m)\n",
    "    print('''# lambda: '''),\n",
    "    print(la)\n",
    "    X,y = init_data_cont(n, m)\n",
    "    w = init_weights(m)\n",
    "    start=time.time()\n",
    "    f,g= simple_two_layers_loss(w, X, y, la)\n",
    "    end=time.time()\n",
    "    print('''# time elapsed: '''),\n",
    "    print(end-start)\n",
    "    print('''# loss value: '''),\n",
    "    print(f)\n",
    "    print('''# gradient: ''')\n",
    "    print(g)\n",
    "\n",
    "def test_assignment_6_3_step7_compare(n, m, la):\n",
    "    print('''### simple_two_layers_loss compare simple loop version and vectorized version ###''')\n",
    "    print('''# number of cases: '''),\n",
    "    print(n)\n",
    "    print('''# number of features: '''),\n",
    "    print(m)\n",
    "    print('''# lambda: '''),\n",
    "    print(la)\n",
    "    X,y = init_data_cont(n, m)\n",
    "    w = init_weights(m)\n",
    "    start=time.time()\n",
    "    f1, g1 = simple_two_layers_loss(w, X, y, la)\n",
    "    end=time.time()\n",
    "    print('''## Result of simple loop version ##''')\n",
    "    print('''# time elapsed: '''),\n",
    "    print(end-start)\n",
    "    print('''# loss value: '''),\n",
    "    print(f1)\n",
    "    print('''# gradient: ''')\n",
    "    print(g1)\n",
    "    start=time.time()\n",
    "    f2, g2 = simple_two_layers_loss_vectorized(w, X, y, la)\n",
    "    end=time.time()\n",
    "    print('''## Result of vectorized version ##''')\n",
    "    print('''# time elapsed: '''),\n",
    "    print(end-start)\n",
    "    print('''# loss value: '''),\n",
    "    print(f2)\n",
    "    print('''# gradient: ''')\n",
    "    print(g2)\n",
    "\n",
    "def test_assignment_6_3_step8_compare(n, m, la, epsilon):\n",
    "    print('''### simple_two_layers_loss compare gradient, derivative version and numerical version ###''')\n",
    "    print('''# number of cases: '''),\n",
    "    print(n)\n",
    "    print('''# number of features: '''),\n",
    "    print(m)\n",
    "    print('''# lambda: '''),\n",
    "    print(la)\n",
    "    print('''# epsilon: '''),\n",
    "    print(epsilon)\n",
    "    X,y = init_data_cont(n, m)\n",
    "    w = init_weights(m)\n",
    "    _, g1 = simple_two_layers_loss_vectorized(w, X, y, la)\n",
    "    func_obj = lambda w : simple_two_layers_loss_vectorized(w, X, y, la)[0]\n",
    "    g2 = numerical_grad(func_obj, w, epsilon)\n",
    "    print('''## Result of derivative version ##''')\n",
    "    print('''# gradient: ''')\n",
    "    print(g1)\n",
    "    print('''## Result of numerical version ##''')\n",
    "    print('''# gradient: ''')\n",
    "    print(g2)\n",
    "    \n",
    "n,m,la=100,10,1.0\n",
    "test_assignment_6_3(n,m,la)\n",
    "n,m,la=100,100000,1.0\n",
    "test_assignment_6_3(n,m,la)\n",
    "n,m,la=100000,10,1.0\n",
    "test_assignment_6_3(n,m,la)\n",
    "n,m,la=100,10,1.0\n",
    "test_assignment_6_3_step7_compare(n, m, la)\n",
    "n,m,la=100,100000,1.0\n",
    "test_assignment_6_3_step7_compare(n, m, la)\n",
    "n,m,la=100000,10,1.0\n",
    "test_assignment_6_3_step7_compare(n, m, la)\n",
    "n,m,la,epsilon=100,10,1.0,0.000001\n",
    "test_assignment_6_3_step8_compare(n, m, la, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6.4 Least Square Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_square_loss(w, X, y, la):\n",
    "    '''\n",
    "    input: w parameter, X variables, y label, la lambda\n",
    "    Assignment 6.4\n",
    "    return f as the loss function value and g as the gradient\n",
    "    '''\n",
    "    n = X.shape[0] # size of dataset\n",
    "    # m = X.shape[1] # number of features\n",
    "    f = 0\n",
    "    g = 0\n",
    "    for i in range(n):\n",
    "        x_cdot_w = np.dot(X[i], w)\n",
    "        f = f + np.square(y[i]-x_cdot_w)\n",
    "        g = g + 2 * (y[i] - x_cdot_w) * -X[i]\n",
    "    f = f + la * np.sum(w*w)\n",
    "    g = g + 2 * la * w.reshape(1,-1)\n",
    "    return f,g\n",
    "\n",
    "def least_square_loss_vectorized(w, X, y, la):\n",
    "    '''\n",
    "    input: w parameter, X variables, y label, la lambda\n",
    "    Assignment 6.4\n",
    "    return f as the loss function value and g as the gradient\n",
    "    '''\n",
    "    n = X.shape[0]\n",
    "    m = w.shape[0]\n",
    "    X_cdot_w = np.matmul(X, w) # n * 1\n",
    "    f = np.sum(np.square( y - X_cdot_w)) + la * np.sum(np.multiply(w, w))\n",
    "    minus_X = -2 * X # n * m\n",
    "    g = np.matmul(minus_X.transpose(), y - X_cdot_w) + 2 * la * w.reshape(-1, 1)\n",
    "    g = g.reshape(1, -1)\n",
    "    return f, g\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Block for Question 6.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### least_square_loss simple loop ###\n",
      "# number of cases:  100\n",
      "# number of features:  10\n",
      "# lambda:  1.0\n",
      "# time elapsed:  0.0018789768219\n",
      "# loss value:  [646.97075866]\n",
      "# gradient: \n",
      "[[246.86514408 287.05028885 264.53088889 261.09061765 229.56289477\n",
      "  262.91284511 260.84003537 248.3841903  270.04947015 279.32944223]]\n",
      "### least_square_loss simple loop ###\n",
      "# number of cases:  100\n",
      "# number of features:  100000\n",
      "# lambda:  1.0\n",
      "# time elapsed:  0.10099196434\n",
      "# loss value:  [6.23971312e+10]\n",
      "# gradient: \n",
      "[[2740576.11937244 2498666.95522744 2256001.2267427  ... 2181312.05481989\n",
      "  2248624.94143555 2358738.22350247]]\n",
      "### least_square_loss simple loop ###\n",
      "# number of cases:  100000\n",
      "# number of features:  10\n",
      "# lambda:  1.0\n",
      "# time elapsed:  1.20673894882\n",
      "# loss value:  [436928.62112624]\n",
      "# gradient: \n",
      "[[213311.13550738 216494.17483098 215687.00251062 199934.66876579\n",
      "  200100.0786276  205478.25046642 202318.58957454 206253.15914215\n",
      "  210255.36541307 208194.54140104]]\n",
      "### least_square_loss compare simple loop version and vectorized version ###\n",
      "# number of cases:  100\n",
      "# number of features:  10\n",
      "# lambda:  1.0\n",
      "## Result of simple loop version ##\n",
      "# time elapsed:  0.00140690803528\n",
      "# loss value:  [619.62913646]\n",
      "# gradient: \n",
      "[[252.60752172 244.75735894 258.24317956 253.77268692 265.09053784\n",
      "  255.90578933 242.28660763 256.51074737 261.65619119 250.67209169]]\n",
      "## Result of vectorized version ##\n",
      "# time elapsed:  0.000218868255615\n",
      "# loss value:  619.6291364631695\n",
      "# gradient: \n",
      "[[252.60752172 244.75735894 258.24317956 253.77268692 265.09053784\n",
      "  255.90578933 242.28660763 256.51074737 261.65619119 250.67209169]]\n",
      "### least_square_loss compare simple loop version and vectorized version ###\n",
      "# number of cases:  100\n",
      "# number of features:  100000\n",
      "# lambda:  1.0\n",
      "## Result of simple loop version ##\n",
      "# time elapsed:  0.0729579925537\n",
      "# loss value:  [6.27322821e+10]\n",
      "# gradient: \n",
      "[[2399487.51253526 2725414.51676297 2300259.39816273 ... 2377437.61895018\n",
      "  2348836.81228258 2539003.76801542]]\n",
      "## Result of vectorized version ##\n",
      "# time elapsed:  0.0297219753265\n",
      "# loss value:  62732282101.75379\n",
      "# gradient: \n",
      "[[2399487.51253526 2725414.51676297 2300259.39816273 ... 2377437.61895018\n",
      "  2348836.81228258 2539003.76801542]]\n",
      "### least_square_loss compare simple loop version and vectorized version ###\n",
      "# number of cases:  100000\n",
      "# number of features:  10\n",
      "# lambda:  1.0\n",
      "## Result of simple loop version ##\n",
      "# time elapsed:  1.18213582039\n",
      "# loss value:  [768470.7060368]\n",
      "# gradient: \n",
      "[[279121.36064673 280189.11794947 285014.35666456 283568.68682971\n",
      "  275685.68638068 276784.38705701 281289.68232565 284046.68405199\n",
      "  273022.55206246 272732.28328518]]\n",
      "## Result of vectorized version ##\n",
      "# time elapsed:  0.00506496429443\n",
      "# loss value:  768470.7060368132\n",
      "# gradient: \n",
      "[[279121.36064673 280189.11794947 285014.35666457 283568.68682972\n",
      "  275685.68638068 276784.38705701 281289.68232565 284046.684052\n",
      "  273022.55206246 272732.28328518]]\n",
      "### least_square_loss compare gradient, derivative version and numerical version ###\n",
      "# number of cases:  100\n",
      "# number of features:  10\n",
      "# lambda:  1.0\n",
      "# epsilon:  1e-06\n",
      "## Result of derivative version ##\n",
      "# gradient: \n",
      "[[101.84622921  99.19468757 109.75452115  97.24298658 110.8453644\n",
      "   93.06611518 104.17264476 100.41039484  99.43869322  96.24219012]]\n",
      "## Result of numerical version ##\n",
      "# gradient: \n",
      "[101.8462292   99.19468757 109.75452116  97.24298658 110.84536439\n",
      "  93.06611518 104.17264475 100.41039484  99.43869322  96.2421901 ]\n"
     ]
    }
   ],
   "source": [
    "# Test Assignment 6.4\n",
    "def test_assignment_6_4(n, m, la):\n",
    "    '''\n",
    "    test the function least_square_loss\n",
    "    n: size of dataset\n",
    "    m: number of features\n",
    "    la: lambda\n",
    "    '''\n",
    "    print('''### least_square_loss simple loop ###''')\n",
    "    print('''# number of cases: '''),\n",
    "    print(n)\n",
    "    print('''# number of features: '''),\n",
    "    print(m)\n",
    "    print('''# lambda: '''),\n",
    "    print(la)\n",
    "    X,y = init_data_cont(n, m)\n",
    "    w = init_weights(m)\n",
    "    start=time.time()\n",
    "    f,g= least_square_loss(w, X, y, la)\n",
    "    end = time.time()\n",
    "    print('''# time elapsed: '''),\n",
    "    print(end-start)\n",
    "    print('''# loss value: '''),\n",
    "    print(f)\n",
    "    print('''# gradient: ''')\n",
    "    print(g)\n",
    "\n",
    "def test_assignment_6_4_step7_compare(n, m, la):\n",
    "    print('''### least_square_loss compare simple loop version and vectorized version ###''')\n",
    "    print('''# number of cases: '''),\n",
    "    print(n)\n",
    "    print('''# number of features: '''),\n",
    "    print(m)\n",
    "    print('''# lambda: '''),\n",
    "    print(la)\n",
    "    X,y = init_data_cont(n, m)\n",
    "    w = init_weights(m)\n",
    "    start=time.time()\n",
    "    f1, g1 = least_square_loss(w, X, y, la)\n",
    "    end = time.time()\n",
    "    print('''## Result of simple loop version ##''')\n",
    "    print('''# time elapsed: '''),\n",
    "    print(end-start)\n",
    "    print('''# loss value: '''),\n",
    "    print(f1)\n",
    "    print('''# gradient: ''')\n",
    "    print(g1)\n",
    "    start=time.time()\n",
    "    f2, g2 = least_square_loss_vectorized(w, X, y, la)\n",
    "    end = time.time()\n",
    "    print('''## Result of vectorized version ##''')\n",
    "    print('''# time elapsed: '''),\n",
    "    print(end-start)\n",
    "    print('''# loss value: '''),\n",
    "    print(f2)\n",
    "    print('''# gradient: ''')\n",
    "    print(g2)\n",
    "\n",
    "def test_assignment_6_4_step8_compare(n, m, la, epsilon):\n",
    "    print('''### least_square_loss compare gradient, derivative version and numerical version ###''')\n",
    "    print('''# number of cases: '''),\n",
    "    print(n)\n",
    "    print('''# number of features: '''),\n",
    "    print(m)\n",
    "    print('''# lambda: '''),\n",
    "    print(la)\n",
    "    print('''# epsilon: '''),\n",
    "    print(epsilon)\n",
    "    X,y = init_data_cont(n, m)\n",
    "    w = init_weights(m)\n",
    "    _, g1 = least_square_loss(w, X, y, la)\n",
    "    func_obj = lambda w : least_square_loss_vectorized(w, X, y, la)[0]\n",
    "    g2 = numerical_grad(func_obj, w, epsilon)\n",
    "    print('''## Result of derivative version ##''')\n",
    "    print('''# gradient: ''')\n",
    "    print(g1)\n",
    "    print('''## Result of numerical version ##''')\n",
    "    print('''# gradient: ''')\n",
    "    print(g2)\n",
    "\n",
    "n,m,la=100,10,1.0\n",
    "test_assignment_6_4(n,m,la)\n",
    "n,m,la=100,100000,1.0\n",
    "test_assignment_6_4(n,m,la)\n",
    "n,m,la=100000,10,1.0\n",
    "test_assignment_6_4(n,m,la)\n",
    "n,m,la=100,10,1.0\n",
    "test_assignment_6_4_step7_compare(n, m, la)\n",
    "n,m,la=100,100000,1.0\n",
    "test_assignment_6_4_step7_compare(n, m, la)\n",
    "n,m,la=100000,10,1.0\n",
    "test_assignment_6_4_step7_compare(n, m, la)\n",
    "n,m,la,epsilon=100,10,1.0,0.000001\n",
    "test_assignment_6_4_step8_compare(n, m, la, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
